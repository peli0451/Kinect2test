\subsection{Gesten und ihre Wirkung}\label{sec:gesten}
Für die eingangs erwähnte Aufgabenstellung war es notwendig, bestimmte Programmfunktionalitäten mit Gesten zu verbinden. Einerseits hätte die Möglichkeit bestanden, mit dem Kinect-eigenen Visual Gesture Builder Gesten aufzunehmen und einzulernen. Diese Gesten werden dann als Datenbank ins Programm geladen und bei Vorführung erkannt. Dies erspart natürlich primitive aber umständliche Low-Level-Erkennungsmechanismen. Weiterhin sind hierdurch einige weiterführende Möglichkeiten gegeben wie etwa die Rückgabe, bis zu welchem Punkt eine Geste bereits ausgeführt wurde (in Bezug zur Gesamtgeste, d.\,h. beispielsweise wieviel Prozent einer Armbewegung vordefinierte Länge bereits ausgeführt wurde). Anderseits wiederum können durch die Kinect-Rohdaten auch eigene Erkennmechanismen implementiert werden. Dies bietet dem Programmierer die vollständige Kontrolle und Freiheit darüber, wie er Gesten definiert und auswertet, statt wie im Falle des Gesture Builders auf ein gewisses Rahmenwerk angewiesen zu sein. Änderungen können kurzfristig und schnell vorgenommen werden und für einfache Projekte ist die Zusatzfunktionalität, die der Visual Gesture Builder gestattet nicht vonnöten, der eher für komplexere Gestenfolgen ausgelegt zu sein scheint. Demgegenüber ist für diese Direktimplementierung von Gesten aber die bereits erwähnte Low-Level-Erkennung zu implementieren, d.\,h. ein Extrahieren von Bewegungen und Bewegungsrichtungen aus den Skelett- und Gelenkdaten, die die Kinect bestimmt. Aus dieser Gegenüberstellung heraus, ist für den gegebenen Einsatzzweck eine direkte Gestenerkennung die sinnvollere Alternative. Mit dem Visual Gesture Builder ist es schlicht nicht möglich, eine wie beabsichtigte $1$"=zu"=$1$"=Abbildung zwischen Geste und Wirkung zu bewerkstelligen.\par 
	Auch bei der Low-Level-Erkennung gibt es jedoch verschiedene Ansätze bzw. Ausprägungen. Es ist sogar das Implementieren nicht ganz primitiver Gestenfolgen möglich, indem eine Geste zeitlich und räumlich in verschiedene Segmente unterteilt wird. Dies sei an einem Beispiel erläutert: Es soll eine Winkgeste der rechten Hand erkannt werden. Die Geste wird in zwei Segmente geteilt. Ein Wechsel zwischen den Segmenten findet statt, wenn die horizontale Position der Hand und des Ellenbogens wechseln. Wird dieser Übergang dreimal in Folge erkannt, so wurde die Winkgeste präsentiert.\par
	Eine genauere Auseinandersetzung mit der Aufgabenstellung und allgemeinen Vorstellungen von intuitiven Gesten für die zu realisierenden Funktionalitäten zeigte jedoch auf, dass auch eine Segmenteinteilung von Gesten für das Projekt nicht notwendig ist. Stattdessen sind die gegebenen Aufgaben (ein Verschieben oder Rotieren per Geste) in ihrer Struktur simpel genug, um die verschiedenen Wirkungen mit diskreten Gesten zu erzeugen, d.\,h. es genügt die Erkennung einer Geste durch bestimmte Zustände der Kinect-Rohdaten zu einem einzigen Zeitpunkt. Um die Wirkung jedoch zu erzielen, ist natürlich auch eine Betrachtung der Geste über mehrere Frames notwendig.\par\medskip
	Im Folgenden ist der im Projekt Verwendung findende Gestenkatalog erklärt. Dabei wird darauf eingegangen, was der Benutzer vorführen muss, damit die Geste erkannt wird und wie die Geste genutzt wird, um in der Anwendung die Kamera oder Objekte zu manipulieren:\par\bigskip
	\begin{description}
		\begin{figure}[H]
		\centering
		\includegraphics[width=.8\textwidth]{pictures/translate_.png}
		\caption{Die (Kamera-)Translations-Geste, mit und ohne eingezeichnetes Skelett und HandStates.}\label{fig:translateg}
		\end{figure}
		\item[TRANSLATE\_GESTURE] (siehe Abb. \ref{fig:translateg})\par
		Der Benutzer hat beide Hände geöffnet, mit den Handflächen zur Kamera (wichtig ist nur, dass die Kinect beide Hände als offen erkennt, die genaue Haltung ist dabei egal). Ein paralleles Verschieben der beiden Hände in eine Richtung bewirkt ein zur Bewegungsgeschwindigkeit proportionales Verschieben der Kamera in diese Richtung.\par 
		Wie in den Vorüberlegungen (Abschnitt \ref{sec:vor}) schon erwähnt ist dies eine einfache Übertragung des Prinzips nachdem eine Person in der Realität einen großen Gegenstand verschieben würde und wird daher der geforderten Intuitivität gerecht.
		\par
		\begin{figure}[H]
		\centering
		\includegraphics[width=.8\textwidth]{pictures/rotate_.png}
		\caption{Die (Kamera-)Rotations-Geste, mit und ohne eingezeichnetes Skelett und HandStates.}\label{fig:rotateg}
		\end{figure}		
		\item[ROTATE\_GESTURE] (siehe Abb. \ref{fig:rotateg})\par
		Der Benutzer hat beide Fäuste geballt. Dann bewirkt eine gleichzeitige Bewegung der Hände auf einer Kreisbahn eine Rotation der Kamera um die Senkrechte des zugehörigen Kreises.
		Analog zur Verschiebegeste fand diese Geste bereits in den Vorüberlegungen (Abschnitt \ref{sec:vor}) Erwähnung. Die Aufgabe, eine Rotation im Raum zu vollführen ist leider weniger alltäglich als die des Verschiebens von Gegenständen. Ein verwandtes Prinzip ist jedoch das des Lenkrades, das man sich --  ins Dreidimensionale erweitert -- als Lenkkugel vorstellen kann. Mittels der üblichen Lenkbewegung kann dann um beliebige Raumachsen rotiert werden. Ähnlich findet dies auch etwa bei der Bedienung von Münzfernrohren statt. Wegen dieser Analogien, kann davon ausgegangen werden, dass der Benutzer schnell ein intuitives Verständnis von der Wirkungsweise dieser Geste entwickelt.
		\par
		\begin{figure}[H]
		\centering
		\includegraphics[width=.8\textwidth]{pictures/grab_.png}
		\caption{Die Objektmanipulations-Geste, mit und ohne eingezeichnetes Skelett und HandStates.}\label{fig:grabg}
		\end{figure}
		\item[GRAB\_GESTURE] (siehe Abb. \ref{fig:grabg})\par
		Zunächst war angedacht, dass die Objektmanipulation dieselben Gesten verwendet wie die Kameramanipulation und die Unterscheidung, was manipuliert wird durch einen globalen Zustand gefällt wird. Bei näherer Betrachtung dieses Ansatzes und ersten Tests dessen fiel auf, dass es so schwierig ist, zwischen Kamera- und Objektmanipulation zu wechseln. Weiterhin schien es während des Testens weniger intuitiv als zuvor angenommen, ein Objekt auf diese Art und Weise zu manipulieren. Es bedarf hier also anderer Ansätze.\par 
		In das Problem der Objektmanipulation eingeschlossen ist das Problem des Object-Pickings, d.\,h. die Auswahl des zu manipulierenden Objekts vom Bildschirm. Auch dies wäre mit der oben beschriebenen Methode, die die Gesten der Kameramanipulation verwendet, nur schwierig und umständlich realisierbar gewesen. Das Object-Picking bietet jedoch einen anderen Weg, einen Ansatz für eine Objektmanipulationsgeste zu finden. Ein alltägliches und dem Benutzer bekanntes Beispiel hierfür ist das einfache Greifen nach einem Gegenstand -- dies motiviert auch den Namen \glqq{}GRAB\grqq{}-Geste. Übertragen in eine Geste ließe sich dies durch eine erhobene und geschlossene Hand definieren. In weiterer Analogie zum Alltagsbeispiel sollte ein Hin- und Herbewegen der Hand, mit der gegriffen wurde (der \glqq Kontroll-Hand\grqq{}) auch das Objekt hin"= und herbewegen. Die Rotation führt jedoch bei dieser Geste zu einem Problem: Mit der geschlossenen Hand ist die Erkennung der Orientierung des entsprechenden Gelenks durch die Kinect zu schlecht, um an dieser Stelle sinnvoll Verwendung zu finden. Im Programm äußerte sich dies einerseits durch ein Ausbleiben der Auswirkungen vorgeführter Rotationen, andererseits aber auch durch ein starkes Rauschen. Probleme dieser Art treten auch an anderer Stelle auf und sind behandelbar (dies wird in Abschnitt \ref{sec:robustheit} deutlich), hier jedoch wurden die Ergebnisse so schlecht, dass eine andere Gestendefinition nötig war. Die Ergebnisse wurden direkt deutlich besser, wenn die GRAB"=Geste durch eine gehobene und offene (!) Hand definiert wurde. Die größere Fläche bietet der Kinect mehr Anhaltspunkte und verbessert so die Genauigkeit für die Orientierung, etwa wenn die Handfläche gekippt bzw. gedreht ist.
		\par
		\begin{figure}[H]
		\centering
		\includegraphics[width=.8\textwidth]{pictures/fly_.png}
		\caption{Die Flug-Geste, mit und ohne eingezeichnetes Skelett und HandStates.}\label{fig:flyg}
		\end{figure}
		\item[FLY\_GESTURE] (siehe Abb. \ref{fig:flyg})\par
		Im Rahmen der Tests mit einem Beispielobjekt wurde schnell deutlich, dass es auch eine einfache Möglichkeit geben sollte, Bewegungen über etwas weitere Strecken durch den Raum zu vollführen, ohne dabei ständig zwischen dem Vorführen einer Geste und einem \glqq Nachgreifen\grqq{} wechseln zu müssen. Eine übliche Lösung für eine solche Aufgabe ist ein Flugmodus. Im konkreten Anwendungsfall sollte dann das Vorführen einer besonderen Geste bewirken, dass die Kamera losfährt und erst anhält, wenn die Geste nicht mehr präsentiert wird.\par 
		Die FLY"=Geste entspricht dem Ausstrecken beider Arme vor den Körper, sodass sich die Hände mehr oder weniger am selben Punkt im 3D"=Raum befinden. Passiv findet bei dieser Geste im Programm eine Bewegung nach vorne statt. Durch Schwenken der Arme soll der Nutzer dabei die Richtung der Bewegung beeinflussen können, d.\,h. ein Zeigen der Arme nach oben bewirkt, dass die Bewegung immer weiter nach oben gezogen wird, während mit einem Zeigen nach links oder rechts eine Kurve geflogen werden kann. Dabei bestimmt der Ausschlag der Arme beim Zeigen (verglichen mit der Ausgangsposition, in der beide Arme genau nach vorne gerichtet sind) die Stärke der Richtungsänderung. Um eine sogenannte Fassrolle durchzuführen oder sich \glqq{}in Kurven legen\grqq{} zu können, kann der Nutzer nebenbei seine Schulterpartie in die entsprechende Richtung kippen. Insgesamt ist die Steuerung des Flugmodus in ihrem Funktionsumfang damit ähnlich zu üblichen Steuerungen von Flugzeugen in Actionspielen oder Simulationen: Es findet eine automatische Bewegung nach vorne statt, die in verschiedene Achsen gekippt werden kann.\par
	Diese erneute Prinzipübertragung macht die Geste intuitiver. Mit Hilfe dieser Fluggeste hat der Benutzer eine im Gegensatzu zur Bewegung über Drehen und Schieben einfache Möglichkeit, sich etwa durch ein System von Gängen in einer 3D-Szene zu bewegen und generell Strecken zurückzulegen, statt Objekte zu betrachten. Werden die Strecken zu lang, kann diese Geste jedoch anstrengend für den Benutzer sein. Bei alternativen Anwendungen wäre dies zusätzlich zu bedenken und eine dem abweichenden Zweck besser angepasste Gestendefinition zu verwenden.
		\par		
		\item[UNKNOWN] Dies enthält alles, was als keine der anderen Gesten erkannt wird. Die Kamera und geladene Objekte sollen, solange diese Geste gezeigt wird, stillstehen.\par
		Neben der naheliegenden Motivation, dass der Nutzer die Szene gegebenenfalls auch bei Ruhe betrachten möchte, dient diese \glqq Geste\grqq{} (oder besser \glqq Nicht-Geste\grqq{}) darüber hinaus noch einem anderen Zweck. Sie kann in andere Gesten, wie beispielsweise Translationen, eingebaut werden, um diese aufzubrechen und \glqq nachgreifen\grqq{} zu können. Erst dies gestattet dem Nutzer, während der Bedienung an Ort und Stelle stehen bleiben zu können.
	\end{description}
	Die Verwendung der Gesten hat ergeben, dass es notwendig ist, bei derartig selbst implementierten Gesten auch eigene Robustheitsmechanismen einzubauen, die die Gestenerkennung gegen Schwankungen der Kinecterkennung (etwa des Status einer Hand) abhärten. Für genauere Informationen hierzu sei auf Abschnitt \ref{sec:robustheit} verwiesen.