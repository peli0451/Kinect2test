	Für unser Vorgehen zentral sind die folgenden beiden Bereiche:
	\begin{enumerate}[label=(\roman*)]
		\item Die Interaktion mit dem Benutzer, d.\,h.
		\begin{itemize}
		\item das Entwerfen intuitiver und eingängiger sowie leicht auszuführender und gut unterscheidbarer Gesten für die verschiedenen Zwecke und
		\item das Auszeichnen einer getrackten Person als \glqq Master\grqq, der das Programm steuert.
		\end{itemize}
		\item Die technische Umsetzung, d.\,h.
		\begin{itemize}
		\item das korrekte Erkennen und Werten von Gesten einer ausgezeichneten getrackten Person,
		\item das Wiedererkennen der ausgezeichneten Person,
		\item das korrekte Berechnen notwendiger Bewegungsparameter bei der Steuerung und
		\item die Einbindung in die bestehende Applikation.
		\end{itemize}
	\end{enumerate}
	Wir stellen in diesem Abschnitt die wichtigen und unmittelbaren Beobachtungen vor, die sich aus der Aufgabenstellung und dem Versuchsaufbau ziehen lassen.\par\bigskip
	Ausgehend von der Aufgabenstellung kann man abstrahierend zwischen zwei primitiven Steuerungsmodi unterscheiden:
	\begin{itemize}
	\item einem Modus, in dem die Kamera verschoben und rotiert werden kann \&
	\item einem Modus, in welchem Objektmanipulationen möglich sind.
	\end{itemize}
	Der Benutzer sollte sich zu jedem Zeitpunkt nur in maximal einem dieser Modi aufhalten, d.\,h. gleichzeitige Kamera- und Objektmanipulationen sind ausgeschlossen. Diese Vereinfachung treffen wir, da damit weniger komplexe Gesten benötigt werden und eine solche simultane Manipulation keine praktische Relevanz besitzt. Für Manipulationen, die man sowohl für die Kamera, als auch für Objekte haben will, bietet dies zudem eine geeignete Kapselung, da z.\,B. Rotationsparameter berechnet werden und dann nur entschieden werden muss, ob sie auf die Kamera oder ein Objekt angewendet werden, je nach Modus. Dies kann, falls so umsetzbar, die Gesamtzahl nötiger Gesten reduzieren.\par\medskip
	Die Kinect ermöglicht ein Tracking des gesamten Körpers für mehrere (genauer sechs) Personen. Wir beschränken uns jedoch nur auf einen Teil dieses Spektrums:
	\begin{itemize}
		\item Wir benötigen nur eine Person, die die Anwendung (möglichst ungestört) steuert. Eine genauere Auswertung der restlichen Personen, ihrer Skelette etc. ist zumeist unnötig.
		\item Die in unserem Anwendungsfall intuitiven Gesten werden ausschließlich mit den Händen (bzw. Armen) durchgeführt. Dazu gibt es einige Constraints in den Gestendefinitionen, die noch eine Reihe weiterer Körpermerkmale verwenden.
		\item Ebenso wird für die Mastererkennung ein bestimmtes Set von Körpermerkmalen verwendet.
	\end{itemize}
	Primitive Erkennungsmöglichkeiten eines Masters kann man etwa aus der Entfernung der getrackten Personen zur Kamera und der Position der Personen im Raum gewinnen, in der oben angedeuteten Vergleichsvariante wird eine Sammlung von Körpermerkmalen zusammengestellt, die später zum Abgleich dienen kann. Genauere Erklärungen folgen weiter unten.\par 
	Intuitive Gesten für Verschiebungen imitieren das Verschieben eines großen Gegenstands, etwa einer imaginären Box, sodass hier etwa ein Verschieben der flachen Hand in der Luft naheliegt. Für eine intuitive Drehgeste eignet sich die Vorstellung eines imaginären Lenkrads, genauer gesagt einer Lenkkugel, bei der die Rotation um eine Raumachse nach dem Lenkradprinzip erfolgt. Eine intuitive Geste zur Objektauswahl ist offenbar eine Greifgeste.\par 
	Die genannten Gesten und Umsetzungen folgten unseren Überlegungen nach ersten Tests und wurden im Laufe des Projekts noch überarbeitet.
	\subsection{Priorisierung der Aufgaben}
	Da eine primitive Mastererkennung nach Entfernung entlang der Tiefenachse leicht zu implementieren ist, konzentriert sich die Arbeit zunächst auf die Gesten, genauer auf die Steuerung der Kamera. Im ersten Schritt sind Gesten und ein Rechenmodell zu entwerfen, das sich zur Steuerung eignet. Dies ist dann für Verschiebung und anschließend Drehung der Kamera zu implementieren. Anschließend findet selbiges für die Objektmanipulation statt. Ist dies in einem zufriedenstellendem Maße gegeben, kann die Mastererkennung ausgebaut werden. Steht die Grundfunktionalität, können Optimierungen und Verbesserungen diskutiert werden.
	\subsection{Hinzugekommene Anforderungen}
	Wie bei solch praktisch orientierten Projekten gewöhnlich, fielen in den Tests und der Arbeit mit dem Programm bestimmte Features auf, die einen äußerst positiven Effekt auf die Bedienerfahrung des Programmes hätten. So kamen zu den in Abschnitt \ref{sec:aufg} genannten Aufgaben zusätzliche Anforderungen hinzu.\par
	In der original angedachten Variante einer Masterfestlegung musste ein Knopf betätigt werden, der den Einspeichervorgang startet. Dies kann ein Partner des Vortragenden am Rechner vornehmen oder aber der Vortragende selbst per Präsentationspointer. Eine Verbesserung dieses Prinzips ist eine Art Selbstauslösermechanik. Dazu soll der vorgesehene Master das Einspeichern am Rechner anstoßen können, das jedoch noch nicht beginnt, solange nicht eine Person (in passender Pose) zum Einspeichern bereit steht. Dies gibt ihm die Möglichkeit, sich ins Aufnahmefeld zu bewegen um dort als Master festgelegt zu werden. Die Notwendigkeit einer zweiten Person, die den Rechner bedient (falls kein Präsentationspointer vorhanden) entfällt damit.\par 
	Ein weiterer Punkt ist das großräumigere Navigieren durch die Szene. Entsprechend der oben genannten Ideen zur Kamerabewegung ist für das Zurücklegen einer größeren Strecke ein ständiges Nachgreifen nötig. Dies ließe sich durch Erschaffen eines \glqq{}Flug-Modus\grqq{} vermeiden. Konzeptuell soll sich die Kamera dabei solange nach vorne bewegen, wie eine bestimmte Geste präsentiert wird. Dazu erscheinen nach vorne ausgestreckte Arme einleuchtend, wobei mit Kippbewegungen und der Zeigerichtung auch die Flugrichtung manipuliert werden kann.\par
	Hinsichtlich der Masterfestlegung fiel weiterhin auf, dass es für den Anwender nützlich ist, die Länge (und damit die Genauigkeit) der Einspeicherung beeinflussen zu können. Dafür eignet es sich, einen Zusammenhang zwischen der Länge des Tastendrucks für Einspeicherung und der Einspeicherungszeit selbst herzustellen.\par
	Schließlich fiel bei der Arbeit schnell auf, dass die Entwicklung einer solchen Anwendung sehr auf visuelles Debugging angewiesen ist. In der Version unseres Programmes das am Ende ausgeliefert werden soll gab es nach den A-priori-Überlegungen keinerlei Rückkopplung zum Hauptprogramm, die über den Erfolg oder Misserfolg von Einzelschritten Auskunft hätte geben können. So kam als Anforderung hinzu, ein Eventsystem in Form von Stubs zu implementieren, das später dazu ausgebaut werden kann, bestimmte Statusmeldungen von der Gestensteuerung und Mastererkennung, wie etwa \glqq{}Master festgelegt\grqq{}, \glqq{}Master verloren\grqq{} oder die Angabe des aktuellen Steuerungsmodus zurückzugeben.
	\subsection{Aufgabenverteilung im Team}
	Das Projekt wurde zu viert begonnen, wobei einer der Teilnehmer gleich zu Beginn wieder absprang. Die Aufgabenverteilung im Team wurde ab der Einarbeitungsphase mit dem Kinect-System dynamisch vorgenommen. So kristallisierten sich im Laufe der Zeit diverse Zuständigkeitsbereiche heraus, die grob so umrissen werden können:
	\begin{description}
		\item[Mario Janke] kümmerte sich vor allem um die Mathematik im Hintergrund, präziser um die diversen Berechnungen von Parametern aus den vorliegenden Kinect-Daten. Als klar wurde, dass das Projekt aufgrund der Eigenschaften der Kinect zusätzliche Robustheitsmechanismen benötigte, wurde das Management unserer Puffer zusätzlicher Aufgabenbereich.
		\item[Peter Lindner] sorgte vorrangig für die Strukturierung des Codes. Dies erstreckt sich auf die Umsetzung des objektorientierten Programmierparadigmas mit der Ausarbeitung und Erstellung der Klassenhierarchie. Die im Zuge dessen entstandene Zustandsmaschine wurde in der Folge von ihm verwaltet. Da für deren Existenz zu großen Teilen das beabsichtigte Wirkungsprinzip der Gesten war, schloss sich dem Aufgabenbereich das Gestenmanagement an.
		\item[Patrick Stäblein] war für das Ansammeln grundlegender Informationen zur Kinect -- gerade in der Anfangsphase -- verantwortlich und programmierte einen Großteil der Mastererkennungsmechanismen in der zweiten Phase des Projekts.
	\end{description}
	Parallel zur Arbeit am Programmcode entstand das vorliegende Dokument und später die Vorbereitung der Abschlusspräsentation. Dabei orientierten sich die vom jeweiligen Projektmitglied bearbeiteten Themengebieten an ihren Zuständigkeiten und damit Wissenschwerpunkten aus der Programmierung.
	\subsection{Werkzeuge}
	In diesem Abschnitt sollen die Tools genannt und erklärt werden, die für die Entwicklung unserer Software vordergründig waren.\par
	Das im Zentrum stehende Trackingsystem \emph{Microsoft Kinect Version 2} war durch das Fachgebiet gegeben. Zur Arbeit damit stand ein Raum bereit, der über die Kinect und einen 3D-Kamera-Aufbau zur Projektion verfügte. Ferner durfte die Kinect für Heimtests auch ausgeliehen werden.\par 
	Mit dem Kinect SDK wird eine Softwarelösung namens \emph{Kinect Studio} ausgeliefert, die sich bei der Kinect-Programmierung zum visuellen Debugging eignet. Im Kinect Studio können die verschiedenen Sensoraufnahmen der Kinect nebst der interpretierten Skelette und Hand-States sowie der festgestellten Tiefensituation betrachtet werden. Diese Features können getoggelt oder umgeschaltet werden. Die Tiefenkarte wird per Falschfarbendarstellung und, sofern erwünscht, sogar dreidimensional präsentiert. Das Kinect Studio erwies sich als sehr hilfreich, um die Güte der Kinect-Daten zu überprüfen und zu erkennen, ob die Kinect einen Teil des Aufnahmebereiches fehlinterpretiert. Dies war zum Teil notwendig, um bei der Programmierung schnell und einfach zwischen Fehlern des Programmes und fehlerhaften Kinect-Daten unterscheiden zu können.\par 
	Ebenfalls zum Kinect-SDK gehörig ist der sogenannte \emph{Visual Gesture Builder}, mit dem Gesten(folgen) aufgenommen und in Programme eingespeist werden können. Da wir uns (s.\,u.) für einen anderen Weg der Gestenimplementierung entschieden haben, fanden hiermit nur kleinere Tests in der Anfangsphase statt.\par 
	Die Kinect-API kann mit JavaScript, C++ oder C\# verwendet werden. Da das uns im Rahmen der Aufgabenstellung übermittelte Programm, für das unsere Gestensteuerung gedacht ist, in C++ geschrieben war, verwendeten wir aus Kompatibilitäts- und Einheitlichkeitsgründen heraus ebenfalls C++ und entwickelten und debuggten mit dem \emph{Microsoft Visual Studio 2015} unter \emph{Microsoft Windows 10}.\par 
	Zur Versionsverwaltung nutzten wir das Kommandozeilentool git mit \href{http://github.com}{GitHub}.\par 